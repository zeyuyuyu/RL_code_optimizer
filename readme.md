# Reward-Guided Code Optimizer  
*Advanced RL Take-Home Assignment — Final Report*

---

## 1 项目概述

现代 LLM 与自动编译优化技术不断进步，但**细粒度 Python 代码优化**依旧高度依赖开发者手动重写。  
本项目实现了一个 **强化学习（RL）智能体**，在确保语义正确的前提下自动重写 Python 函数，以提升 **代码简洁性** 与 **运行性能**。整个流程被建模为「代码 → 代码」的序列决策问题：  
智能体对源码执行一系列 *AST* 级变换，每一步依据奖励信号评估成效并更新策略。

---

## 2 任务与语料

| 函数 | 行数 | 待优化模式 |
|------|------|------------|
| `sum_list` | 4 | 循环累加 → `sum()` |
| `max_list` | 5 | 循环取最大 → `max()` |
| `check_positive` | 4 | `if/else` → 布尔表达式 |
| `greet` | 5 | 冗余 docstring / 变量 |
| `double_list` | 5 | 循环 `append` → 列表推导 |

- **单元测试**：为每个函数准备覆盖典型边界的输入–输出用例，确保语义一致。  
- **性能基准**：为每个函数设置固定输入，用 `time.perf_counter` 连续测 3 次取均值。

---

## 3 代码变换代理

### 动作空间（6 种）

| ID | 变换 | 典型收益 |
|----|------|----------|
| 0 | 删除 docstring | 缩短 |
| 1 | 变量重命名（最长 → 单字符） | 缩短 |
| 2 | 循环累加 → `sum()` | 缩短 × 加速 |
| 3 | 循环最大 → `max()` | 同上 |
| 4 | `if True/False` → 布尔表达式 | 缩短 |
| 5 | `append` 循环 → 列表推导 | 缩短 × 小幅加速 |

全部变换在 **AST** 层面模式匹配，保证生成代码语法合法且功能等价。

---

## 4 奖励设计

\[
R = (\text{len}_{\text{before}}-\text{len}_{\text{after}})\;+\;20\!\times\!\bigl(\tfrac{t_{\text{before}}}{t_{\text{after}}}-1\bigr)
\]

- **长度奖励**：每减少 1 字符记 +1  
- **速度奖励**：速度提升比例 × 20（经网格搜索得到的折衷权重）  
- **正确性惩罚**：若任一单元测试失败，立即记 `R = −10` 并终止 episode  
- **步长限制**：单 episode 最多 10 步，防止无界搜索

> 设计原则：**先保证正确性 → 再奖励性能**。  
> 权重 20 使毫秒级时间提升与字符量级减幅在同一尺度内可比较。

---

## 5 强化学习算法

| 项目 | 配置 |
|------|------|
| 状态特征 | 代码长度归一化、是否含 docstring、形参数量 |
| 策略网络 | 3 维输入 → MLP(64-64) → 6-维 softmax |
| 算法 | REINFORCE + batch baseline（10 轨迹） |
| 探索 | 动作按策略概率采样，避免早期过度贪心 |
| 更新 | 每 10 episode 批量梯度下降，使用均值基线降方差 |

---

## 6 实验结果

### 6.1 学习曲线  

（图示为每 10 episode 平均回报，收敛至 ≈ 50）


### 6.2 优化前后指标

| 函数 | 长度 (字符) | 速度 (ms) |
|------|-------------|-----------|
| `sum_list` | 53 → **27** | 0.449 → **0.086** (-80 %) |
| `max_list` | 91 → **27** | 0.285 → 0.245 (-14 %) |
| `check_positive` | 53 → **30** | ≈ 持平 |
| `greet` | 94 → **38** | ≈ 持平 |
| `double_list` | 69 → **36** | ± 3 %（计时噪声） |

---

## 7 案例示例

| 示例 | 关键变化 | 效果 |
|------|----------|------|
| **sum_list** | 循环→`sum()` | 长度-49 %，速度-80 % |
| **check_positive** | `if/else`→布尔表达式 | 长度-43 %，速度基本不变 |

---

## 8 观察与讨论

1. **Reward Hacking**  
   智能体常先执行变量重命名再替换循环，两步累计更高奖励——典型「分步套利」。  
2. **方差控制**  
   使用 batch-mean baseline 后，学习曲线平滑且收敛提速约 30 %。  
3. **泛化能力**  
   通用规则（docstring 删除、变量重命名）在未见函数上仍有效；循环→内建替换需显式模式或更大语料。  
4. **计时噪声**  
   对极短流程 (<0.01 ms) 计时浮动显著，可通过增大输入或多次取平均减噪。

---

## 9 结论与未来工作

- **可行性验证**：简单 REINFORCE + 规则动作即可显著精简代码并在部分场景提速。  
- **局限**：动作空间依赖人工模式；奖励权重需调参；对真实大型代码库泛化待验证。  
- **展望**  
  1. 联合 LLM 生成候选变换，引入自监督筛选，扩展动作多样性  
  2. 融合更多度量（内存、静态安全、可读性）构造多目标奖励  
  3. 迁移至 CodeSearchNet 等大规模数据集，评估通用性和鲁棒性

---

*Report generated {{DATE}}*

